2021-06-17 15:02:05,595 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-06-17 15:02:05,596 - INFO - joeynmt.data - Loading training data...
2021-06-17 15:02:05,759 - INFO - joeynmt.data - Building vocabulary...
2021-06-17 15:02:05,760 - INFO - joeynmt.data - Loading dev data...
2021-06-17 15:02:05,761 - INFO - joeynmt.data - Loading test data...
2021-06-17 15:02:05,761 - INFO - joeynmt.data - Data loaded.
2021-06-17 15:02:05,780 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-06-17 15:02:06,257 - INFO - joeynmt.model - Enc-dec model built.
2021-06-17 15:02:06,263 - INFO - joeynmt.training - Total params: 44432384
2021-06-17 15:02:06,264 - DEBUG - joeynmt.training - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'decoder.to_embed.weight', 'decoder.to_out.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'tag_embed.lut.weight', 'trg_embed.lut.weight']
2021-06-17 15:02:06,265 - INFO - joeynmt.helpers - cfg.name                           : ccg_transformer
2021-06-17 15:02:06,265 - INFO - joeynmt.helpers - cfg.data.src                       : de
2021-06-17 15:02:06,265 - INFO - joeynmt.helpers - cfg.data.trg                       : en
2021-06-17 15:02:06,265 - INFO - joeynmt.helpers - cfg.data.tag                       : tags
2021-06-17 15:02:06,265 - INFO - joeynmt.helpers - cfg.data.train                     : ../data/mini/train_mini
2021-06-17 15:02:06,265 - INFO - joeynmt.helpers - cfg.data.dev                       : ../data/mini/dev_mini
2021-06-17 15:02:06,265 - INFO - joeynmt.helpers - cfg.data.test                      : ../data/mini/test_mini
2021-06-17 15:02:06,266 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-06-17 15:02:06,266 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-06-17 15:02:06,266 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100
2021-06-17 15:02:06,266 - INFO - joeynmt.helpers - cfg.data.random_train_subset       : 10
2021-06-17 15:02:06,266 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-06-17 15:02:06,266 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-06-17 15:02:06,266 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-06-17 15:02:06,266 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-06-17 15:02:06,266 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens
2021-06-17 15:02:06,266 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]
2021-06-17 15:02:06,266 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-06-17 15:02:06,266 - INFO - joeynmt.helpers - cfg.training.patience              : 8
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0002
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.batch_size            : 100
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.batch_type            : token
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 100
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.epochs                : 10
2021-06-17 15:02:06,267 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 25
2021-06-17 15:02:06,268 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 10
2021-06-17 15:02:06,268 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-06-17 15:02:06,268 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/ccg_transformer_test
2021-06-17 15:02:06,268 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-06-17 15:02:06,268 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-06-17 15:02:06,268 - INFO - joeynmt.helpers - cfg.training.use_cuda              : False
2021-06-17 15:02:06,268 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100
2021-06-17 15:02:06,269 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]
2021-06-17 15:02:06,269 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3
2021-06-17 15:02:06,269 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-06-17 15:02:06,269 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-06-17 15:02:06,269 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-06-17 15:02:06,269 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-06-17 15:02:06,269 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-06-17 15:02:06,269 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : False
2021-06-17 15:02:06,269 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True
2021-06-17 15:02:06,269 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-06-17 15:02:06,269 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-06-17 15:02:06,269 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 8
2021-06-17 15:02:06,270 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-06-17 15:02:06,270 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-06-17 15:02:06,270 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0
2021-06-17 15:02:06,270 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-06-17 15:02:06,270 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 2048
2021-06-17 15:02:06,270 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.1
2021-06-17 15:02:06,270 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-06-17 15:02:06,270 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-06-17 15:02:06,270 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 8
2021-06-17 15:02:06,270 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-06-17 15:02:06,270 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-06-17 15:02:06,270 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0
2021-06-17 15:02:06,271 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512
2021-06-17 15:02:06,271 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 2048
2021-06-17 15:02:06,271 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.1
2021-06-17 15:02:06,271 - INFO - joeynmt.helpers - cfg.model.decoder.use_tags         : True
2021-06-17 15:02:06,271 - INFO - joeynmt.helpers - cfg.model.decoder.tag_embeddings.embedding_dim : 128
2021-06-17 15:02:06,271 - INFO - joeynmt.helpers - cfg.model.decoder.tag_embeddings.scale : True
2021-06-17 15:02:06,271 - INFO - joeynmt.helpers - cfg.model.decoder.tag_embeddings.dropout : 0.0
2021-06-17 15:02:06,271 - INFO - joeynmt.helpers - Data set sizes: 
	train 10,
	valid 5,
	test 5
2021-06-17 15:02:06,271 - INFO - joeynmt.helpers - First training example:
	[SRC] beide Finanzierungs@@ operationen haben ihren Schwerpunkt auf der Finanzierung von KMU in einem Ziel @-@ 1 @-@ Gebiet der Europäischen Union und werden daher von der EIB als strategisch wichtig angesehen .
	[TRG] both operations are considered strategically important for EIB given their focus on the financing of SMEs in an objective 1 region of the European Union .
	[TAG] NP/N N (S[dcl]\NP)/(S[pss]\NP) (S[pss]\NP)/(S[adj]\NP) (S[adj]\NP)/(S[adj]\NP) S[adj]\NP ((S\NP)\(S\NP))/NP N ((S\NP)\(S\NP))/NP NP/(N/PP) (N/PP)/PP PP/NP NP/N N/PP PP/NP N (N\N)/NP NP/N N/N N/N N/PP PP/NP NP/N N/N N .
2021-06-17 15:02:06,271 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) der (6) die (7) , (8) @-@ (9) und
2021-06-17 15:02:06,271 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) . (6) of (7) , (8) and (9) for
2021-06-17 15:02:06,271 - INFO - joeynmt.helpers - First 10 words (tag): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) N (5) N/N (6) NP/N (7) PP/NP (8) . (9) conj
2021-06-17 15:02:06,272 - INFO - joeynmt.helpers - Number of Src words (types): 157
2021-06-17 15:02:06,272 - INFO - joeynmt.helpers - Number of Trg words (types): 146
2021-06-17 15:02:06,272 - INFO - joeynmt.helpers - Number of Tags (types): 44
2021-06-17 15:02:06,272 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=157),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=146),
	tag_embed=Embeddings(embedding_dim=128, vocab_size=44),
   )
2021-06-17 15:02:06,273 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 100
	total batch size (w. parallel & accumulation): 100
2021-06-17 15:02:06,273 - INFO - joeynmt.training - EPOCH 1
2021-06-17 15:02:07,586 - INFO - joeynmt.training - Epoch   1: total training loss 42.44
2021-06-17 15:02:07,587 - INFO - joeynmt.training - EPOCH 2
2021-06-17 15:02:08,819 - INFO - joeynmt.training - Epoch   2: total training loss 30.01
2021-06-17 15:02:08,819 - INFO - joeynmt.training - EPOCH 3
2021-06-17 15:02:09,379 - INFO - joeynmt.training - Epoch   3, Step:       10, Batch Loss:     7.138929, Tokens per Sec:      158, Lr: 0.000200
2021-06-17 15:02:10,125 - INFO - joeynmt.training - Epoch   3: total training loss 27.01
2021-06-17 15:02:10,125 - INFO - joeynmt.training - EPOCH 4
2021-06-17 15:02:11,344 - INFO - joeynmt.training - Epoch   4: total training loss 25.80
2021-06-17 15:02:11,344 - INFO - joeynmt.training - EPOCH 5
2021-06-17 15:02:12,600 - INFO - joeynmt.training - Epoch   5, Step:       20, Batch Loss:     6.276785, Tokens per Sec:      194, Lr: 0.000200
2021-06-17 15:02:12,600 - INFO - joeynmt.training - Epoch   5: total training loss 25.09
2021-06-17 15:02:12,601 - INFO - joeynmt.training - EPOCH 6
2021-06-17 15:02:13,929 - INFO - joeynmt.training - Epoch   6: total training loss 24.73
2021-06-17 15:02:13,929 - INFO - joeynmt.training - EPOCH 7
2021-06-17 15:02:26,962 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2021-06-17 15:02:26,963 - INFO - joeynmt.training - Saving new checkpoint.
2021-06-17 15:02:27,589 - INFO - joeynmt.training - Example #0
2021-06-17 15:02:27,589 - DEBUG - joeynmt.training - 	Raw source:     ['<@@', 'sr@@', 'c@@', 'set', 'seti@@', 'd@@', '=@@', '"@@', 'new@@', 'st@@', 'est@@', '201@@', '7@@', '"', 'sr@@', 'c@@', 'lang@@', '=@@', '"@@', 'any@@', '"@@', '>']
2021-06-17 15:02:27,589 - DEBUG - joeynmt.training - 	Raw hypothesis: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']
2021-06-17 15:02:27,589 - INFO - joeynmt.training - 	Source:     <srcset setid="newstest2017" srclang="any">
2021-06-17 15:02:27,589 - INFO - joeynmt.training - 	Reference:  <refset setid="newstest2017" srclang="any" trglang="en">
2021-06-17 15:02:27,589 - INFO - joeynmt.training - 	Hypothesis: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2021-06-17 15:02:27,589 - INFO - joeynmt.training - Example #1
2021-06-17 15:02:27,589 - DEBUG - joeynmt.training - 	Raw source:     ['<@@', 'doc', 'sy@@', 'si@@', 'd@@', '=@@', '"@@', 'ref@@', '"', 'do@@', 'ci@@', 'd@@', '=@@', '"@@', 'ab@@', 'c@@', 'new@@', 's.@@', '199@@', '76@@', '2@@', '"', 'gen@@', 're@@', '=@@', '"@@', 'news@@', '"', 'ori@@', 'g@@', 'lang@@', '=@@', '"@@', 'en@@', '"@@', '>']
2021-06-17 15:02:27,589 - DEBUG - joeynmt.training - 	Raw hypothesis: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']
2021-06-17 15:02:27,589 - INFO - joeynmt.training - 	Source:     <doc sysid="ref" docid="abcnews.199762" genre="news" origlang="en">
2021-06-17 15:02:27,590 - INFO - joeynmt.training - 	Reference:  <doc sysid="ref" docid="abcnews.199762" genre="news" origlang="en">
2021-06-17 15:02:27,590 - INFO - joeynmt.training - 	Hypothesis: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2021-06-17 15:02:27,590 - INFO - joeynmt.training - Example #2
2021-06-17 15:02:27,590 - DEBUG - joeynmt.training - 	Raw source:     ['<@@', 'p@@', '>']
2021-06-17 15:02:27,590 - DEBUG - joeynmt.training - 	Raw hypothesis: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']
2021-06-17 15:02:27,590 - INFO - joeynmt.training - 	Source:     <p>
2021-06-17 15:02:27,590 - INFO - joeynmt.training - 	Reference:  <p>
2021-06-17 15:02:27,590 - INFO - joeynmt.training - 	Hypothesis: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2021-06-17 15:02:27,590 - INFO - joeynmt.training - Example #3
2021-06-17 15:02:27,590 - DEBUG - joeynmt.training - 	Raw source:     ['<@@', 'se@@', 'g', 'id@@', '=@@', '"@@', '1@@', '"@@', '>@@', '28@@', '-@@', 'jähriger', 'Koch', 'in', 'San', 'Francisco', 'Mall', 'tot', 'aufge@@', 'fun@@', 'den@@', '<@@', '/@@', 'seg@@', '>']
2021-06-17 15:02:27,590 - DEBUG - joeynmt.training - 	Raw hypothesis: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']
2021-06-17 15:02:27,590 - INFO - joeynmt.training - 	Source:     <seg id="1">28-jähriger Koch in San Francisco Mall tot aufgefunden</seg>
2021-06-17 15:02:27,591 - INFO - joeynmt.training - 	Reference:  <seg id="1">28-Year-Old Chef Found Dead at San Francisco Mall</seg>
2021-06-17 15:02:27,591 - INFO - joeynmt.training - 	Hypothesis: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2021-06-17 15:02:27,591 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step       25: bleu:   0.07, loss: 539.6097, ppl:  47.1982, duration: 13.3924s
2021-06-17 15:02:28,763 - INFO - joeynmt.training - Epoch   7: total training loss 24.39
2021-06-17 15:02:28,763 - INFO - joeynmt.training - EPOCH 8
2021-06-17 15:02:29,420 - INFO - joeynmt.training - Epoch   8, Step:       30, Batch Loss:     5.869115, Tokens per Sec:      169, Lr: 0.000200
2021-06-17 15:02:30,108 - INFO - joeynmt.training - Epoch   8: total training loss 23.83
2021-06-17 15:02:30,108 - INFO - joeynmt.training - EPOCH 9
2021-06-17 15:02:31,442 - INFO - joeynmt.training - Epoch   9: total training loss 23.35
2021-06-17 15:02:31,443 - INFO - joeynmt.training - EPOCH 10
2021-06-17 15:02:32,808 - INFO - joeynmt.training - Epoch  10, Step:       40, Batch Loss:     5.652222, Tokens per Sec:      178, Lr: 0.000200
2021-06-17 15:02:32,808 - INFO - joeynmt.training - Epoch  10: total training loss 22.68
2021-06-17 15:02:32,808 - INFO - joeynmt.training - Training ended after  10 epochs.
2021-06-17 15:02:32,808 - INFO - joeynmt.training - Best validation result (greedy) at step       25:  47.20 ppl.
2021-06-17 15:02:32,824 - INFO - joeynmt.prediction - Process device: cpu, n_gpu: 0, batch_size per device: 500 (with beam_size)
2021-06-17 15:02:33,116 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-06-17 15:02:33,653 - INFO - joeynmt.model - Enc-dec model built.
2021-06-17 15:02:33,684 - INFO - joeynmt.prediction - Decoding on dev set (../data/mini/dev_mini.en)...
2021-06-17 15:02:35,514 - INFO - joeynmt.prediction -  dev bleu[13a]:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-06-17 15:02:35,515 - INFO - joeynmt.prediction - Translations saved to: models/ccg_transformer_test/00000025.hyps.dev
2021-06-17 15:02:35,515 - INFO - joeynmt.prediction - Decoding on test set (../data/mini/test_mini.en)...
2021-06-17 15:02:37,179 - INFO - joeynmt.prediction - test bleu[13a]:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-06-17 15:02:37,179 - INFO - joeynmt.prediction - Translations saved to: models/ccg_transformer_test/00000025.hyps.test
