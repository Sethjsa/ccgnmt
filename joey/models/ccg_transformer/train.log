2021-06-16 11:07:33,610 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-06-16 11:07:33,611 - INFO - joeynmt.data - Loading training data...
2021-06-16 11:07:33,782 - INFO - joeynmt.data - Building vocabulary...
2021-06-16 11:07:33,784 - INFO - joeynmt.data - Loading dev data...
2021-06-16 11:07:33,818 - INFO - joeynmt.data - Loading test data...
2021-06-16 11:07:33,847 - INFO - joeynmt.data - Data loaded.
2021-06-16 11:07:33,864 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-06-16 11:07:34,348 - INFO - joeynmt.model - Enc-dec model built.
2021-06-16 11:07:34,353 - INFO - joeynmt.training - Total params: 44306460
2021-06-16 11:07:34,354 - DEBUG - joeynmt.training - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'decoder.tag_dec_att.key_layer.weight', 'decoder.to_embed.weight', 'decoder.to_out.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'tag_embed.lut.weight', 'trg_embed.lut.weight']
2021-06-16 11:07:34,355 - INFO - joeynmt.helpers - cfg.name                           : ccg_transformer
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.data.src                       : de
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.data.trg                       : en
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.data.tag                       : tags
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.data.train                     : ../data/mini/train_mini
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.data.dev                       : ../data/full/dev_bpe
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.data.test                      : ../data/full/test_bpe
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.data.random_train_subset       : 10
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-06-16 11:07:34,356 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-06-16 11:07:34,357 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens
2021-06-16 11:07:34,357 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]
2021-06-16 11:07:34,357 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-06-16 11:07:34,357 - INFO - joeynmt.helpers - cfg.training.patience              : 8
2021-06-16 11:07:34,357 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7
2021-06-16 11:07:34,357 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy
2021-06-16 11:07:34,357 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0002
2021-06-16 11:07:34,357 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08
2021-06-16 11:07:34,357 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-06-16 11:07:34,358 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1
2021-06-16 11:07:34,358 - INFO - joeynmt.helpers - cfg.training.batch_size            : 100
2021-06-16 11:07:34,358 - INFO - joeynmt.helpers - cfg.training.batch_type            : token
2021-06-16 11:07:34,358 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 100
2021-06-16 11:07:34,358 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token
2021-06-16 11:07:34,358 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1
2021-06-16 11:07:34,358 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2021-06-16 11:07:34,358 - INFO - joeynmt.helpers - cfg.training.epochs                : 100
2021-06-16 11:07:34,358 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 4000
2021-06-16 11:07:34,358 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100
2021-06-16 11:07:34,358 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-06-16 11:07:34,358 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/ccg_transformer
2021-06-16 11:07:34,359 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-06-16 11:07:34,359 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-06-16 11:07:34,359 - INFO - joeynmt.helpers - cfg.training.use_cuda              : False
2021-06-16 11:07:34,359 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100
2021-06-16 11:07:34,359 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]
2021-06-16 11:07:34,359 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3
2021-06-16 11:07:34,359 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-06-16 11:07:34,359 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-06-16 11:07:34,360 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-06-16 11:07:34,360 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-06-16 11:07:34,360 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-06-16 11:07:34,360 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : False
2021-06-16 11:07:34,360 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True
2021-06-16 11:07:34,360 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-06-16 11:07:34,360 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-06-16 11:07:34,360 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 8
2021-06-16 11:07:34,360 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-06-16 11:07:34,360 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-06-16 11:07:34,360 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0
2021-06-16 11:07:34,361 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-06-16 11:07:34,361 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 2048
2021-06-16 11:07:34,361 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.1
2021-06-16 11:07:34,361 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-06-16 11:07:34,361 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-06-16 11:07:34,361 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 8
2021-06-16 11:07:34,362 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-06-16 11:07:34,362 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-06-16 11:07:34,362 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0
2021-06-16 11:07:34,362 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512
2021-06-16 11:07:34,362 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 2048
2021-06-16 11:07:34,362 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.1
2021-06-16 11:07:34,362 - INFO - joeynmt.helpers - cfg.model.decoder.use_tags         : True
2021-06-16 11:07:34,362 - INFO - joeynmt.helpers - cfg.model.decoder.tag_embeddings.embedding_dim : 10
2021-06-16 11:07:34,362 - INFO - joeynmt.helpers - cfg.model.decoder.tag_embeddings.scale : True
2021-06-16 11:07:34,363 - INFO - joeynmt.helpers - cfg.model.decoder.tag_embeddings.dropout : 0.0
2021-06-16 11:07:34,363 - INFO - joeynmt.helpers - Data set sizes: 
	train 10,
	valid 3526,
	test 2582
2021-06-16 11:07:34,363 - INFO - joeynmt.helpers - First training example:
	[SRC] beide Finanzierungs@@ operationen haben ihren Schwerpunkt auf der Finanzierung von KMU in einem Ziel @-@ 1 @-@ Gebiet der Europ√§ischen Union und werden daher von der EIB als strategisch wichtig angesehen .
	[TRG] both operations are considered strategically important for EIB given their focus on the financing of SMEs in an objective 1 region of the European Union .
	[TAG] NP/N N (S[dcl]\NP)/(S[pss]\NP) (S[pss]\NP)/(S[adj]\NP) (S[adj]\NP)/(S[adj]\NP) S[adj]\NP ((S\NP)\(S\NP))/NP N ((S\NP)\(S\NP))/NP NP/(N/PP) (N/PP)/PP PP/NP NP/N N/PP PP/NP N (N\N)/NP NP/N N/N N/N N/PP PP/NP NP/N N/N N .
2021-06-16 11:07:34,363 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) der (6) die (7) , (8) @-@ (9) und
2021-06-16 11:07:34,363 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) . (6) of (7) , (8) and (9) for
2021-06-16 11:07:34,364 - INFO - joeynmt.helpers - First 10 words (tag): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) N (5) N/N (6) NP/N (7) PP/NP (8) . (9) conj
2021-06-16 11:07:34,364 - INFO - joeynmt.helpers - Number of Src words (types): 157
2021-06-16 11:07:34,364 - INFO - joeynmt.helpers - Number of Trg words (types): 146
2021-06-16 11:07:34,364 - INFO - joeynmt.helpers - Number of Tags (types): 44
2021-06-16 11:07:34,364 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=157),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=146),
	tag_embed=Embeddings(embedding_dim=10, vocab_size=44),
   )
2021-06-16 11:07:34,364 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 100
	total batch size (w. parallel & accumulation): 100
2021-06-16 11:07:34,365 - INFO - joeynmt.training - EPOCH 1
2021-06-16 11:07:35,653 - INFO - joeynmt.training - Epoch   1: total training loss 25.06
2021-06-16 11:07:35,653 - INFO - joeynmt.training - EPOCH 2
2021-06-16 11:07:36,778 - INFO - joeynmt.training - Epoch   2: total training loss 16.65
2021-06-16 11:07:36,778 - INFO - joeynmt.training - EPOCH 3
2021-06-16 11:07:38,062 - INFO - joeynmt.training - Epoch   3: total training loss 15.94
2021-06-16 11:07:38,062 - INFO - joeynmt.training - EPOCH 4
2021-06-16 11:07:39,355 - INFO - joeynmt.training - Epoch   4: total training loss 15.41
2021-06-16 11:07:39,355 - INFO - joeynmt.training - EPOCH 5
2021-06-16 11:07:40,687 - INFO - joeynmt.training - Epoch   5: total training loss 15.15
2021-06-16 11:07:40,687 - INFO - joeynmt.training - EPOCH 6
2021-06-16 11:07:42,033 - INFO - joeynmt.training - Epoch   6: total training loss 14.97
2021-06-16 11:07:42,033 - INFO - joeynmt.training - EPOCH 7
2021-06-16 11:07:43,201 - INFO - joeynmt.training - Epoch   7: total training loss 14.74
2021-06-16 11:07:43,201 - INFO - joeynmt.training - EPOCH 8
2021-06-16 11:07:44,696 - INFO - joeynmt.training - Epoch   8: total training loss 14.48
2021-06-16 11:07:44,696 - INFO - joeynmt.training - EPOCH 9
2021-06-16 11:07:46,441 - INFO - joeynmt.training - Epoch   9: total training loss 14.14
2021-06-16 11:07:46,441 - INFO - joeynmt.training - EPOCH 10
2021-06-16 11:07:47,922 - INFO - joeynmt.training - Epoch  10: total training loss 13.72
2021-06-16 11:07:47,922 - INFO - joeynmt.training - EPOCH 11
2021-06-16 11:07:49,349 - INFO - joeynmt.training - Epoch  11: total training loss 12.69
2021-06-16 11:07:49,349 - INFO - joeynmt.training - EPOCH 12
2021-06-16 11:07:50,784 - INFO - joeynmt.training - Epoch  12: total training loss 11.65
2021-06-16 11:07:50,784 - INFO - joeynmt.training - EPOCH 13
2021-06-16 11:07:51,985 - INFO - joeynmt.training - Epoch  13: total training loss 10.25
2021-06-16 11:07:51,985 - INFO - joeynmt.training - EPOCH 14
2021-06-16 11:07:53,306 - INFO - joeynmt.training - Epoch  14: total training loss 8.91
2021-06-16 11:07:53,306 - INFO - joeynmt.training - EPOCH 15
2021-06-16 11:07:54,865 - INFO - joeynmt.training - Epoch  15: total training loss 7.57
2021-06-16 11:07:54,865 - INFO - joeynmt.training - EPOCH 16
2021-06-16 11:07:56,480 - INFO - joeynmt.training - Epoch  16: total training loss 6.46
2021-06-16 11:07:56,480 - INFO - joeynmt.training - EPOCH 17
2021-06-16 11:07:57,975 - INFO - joeynmt.training - Epoch  17: total training loss 5.55
2021-06-16 11:07:57,975 - INFO - joeynmt.training - EPOCH 18
2021-06-16 11:07:59,630 - INFO - joeynmt.training - Epoch  18: total training loss 3.96
2021-06-16 11:07:59,630 - INFO - joeynmt.training - EPOCH 19
2021-06-16 11:08:01,207 - INFO - joeynmt.training - Epoch  19: total training loss 3.36
2021-06-16 11:08:01,207 - INFO - joeynmt.training - EPOCH 20
2021-06-16 11:08:02,559 - INFO - joeynmt.training - Epoch  20: total training loss 2.28
2021-06-16 11:08:02,559 - INFO - joeynmt.training - EPOCH 21
2021-06-16 11:08:03,918 - INFO - joeynmt.training - Epoch  21: total training loss 1.69
2021-06-16 11:08:03,918 - INFO - joeynmt.training - EPOCH 22
2021-06-16 11:08:05,223 - INFO - joeynmt.training - Epoch  22: total training loss 1.45
2021-06-16 11:08:05,224 - INFO - joeynmt.training - EPOCH 23
2021-06-16 11:08:06,801 - INFO - joeynmt.training - Epoch  23: total training loss 1.25
2021-06-16 11:08:06,801 - INFO - joeynmt.training - EPOCH 24
2021-06-16 11:08:08,240 - INFO - joeynmt.training - Epoch  24: total training loss 1.08
2021-06-16 11:08:08,241 - INFO - joeynmt.training - EPOCH 25
2021-06-16 11:08:09,579 - INFO - joeynmt.training - Epoch  25, Step:      100, Batch Loss:     0.161245, Tokens per Sec:      182, Lr: 0.000200
2021-06-16 11:08:09,579 - INFO - joeynmt.training - Epoch  25: total training loss 0.98
2021-06-16 11:08:09,580 - INFO - joeynmt.training - EPOCH 26
2021-06-16 11:08:10,894 - INFO - joeynmt.training - Epoch  26: total training loss 0.88
2021-06-16 11:08:10,894 - INFO - joeynmt.training - EPOCH 27
2021-06-16 11:08:12,184 - INFO - joeynmt.training - Epoch  27: total training loss 0.87
2021-06-16 11:08:12,184 - INFO - joeynmt.training - EPOCH 28
2021-06-16 11:08:13,434 - INFO - joeynmt.training - Epoch  28: total training loss 0.74
2021-06-16 11:08:13,434 - INFO - joeynmt.training - EPOCH 29
2021-06-16 11:08:14,721 - INFO - joeynmt.training - Epoch  29: total training loss 0.77
2021-06-16 11:08:14,721 - INFO - joeynmt.training - EPOCH 30
2021-06-16 11:08:16,012 - INFO - joeynmt.training - Epoch  30: total training loss 0.65
2021-06-16 11:08:16,012 - INFO - joeynmt.training - EPOCH 31
2021-06-16 11:08:17,259 - INFO - joeynmt.training - Epoch  31: total training loss 0.65
2021-06-16 11:08:17,259 - INFO - joeynmt.training - EPOCH 32
2021-06-16 11:08:18,537 - INFO - joeynmt.training - Epoch  32: total training loss 0.56
2021-06-16 11:08:18,537 - INFO - joeynmt.training - EPOCH 33
2021-06-16 11:08:19,784 - INFO - joeynmt.training - Epoch  33: total training loss 0.56
2021-06-16 11:08:19,784 - INFO - joeynmt.training - EPOCH 34
2021-06-16 11:08:21,050 - INFO - joeynmt.training - Epoch  34: total training loss 0.53
2021-06-16 11:08:21,051 - INFO - joeynmt.training - EPOCH 35
2021-06-16 11:08:22,297 - INFO - joeynmt.training - Epoch  35: total training loss 0.52
2021-06-16 11:08:22,297 - INFO - joeynmt.training - EPOCH 36
2021-06-16 11:08:23,674 - INFO - joeynmt.training - Epoch  36: total training loss 0.49
2021-06-16 11:08:23,674 - INFO - joeynmt.training - EPOCH 37
2021-06-16 11:08:25,191 - INFO - joeynmt.training - Epoch  37: total training loss 0.51
2021-06-16 11:08:25,191 - INFO - joeynmt.training - EPOCH 38
2021-06-16 11:08:26,576 - INFO - joeynmt.training - Epoch  38: total training loss 0.48
2021-06-16 11:08:26,576 - INFO - joeynmt.training - EPOCH 39
2021-06-16 11:08:27,841 - INFO - joeynmt.training - Epoch  39: total training loss 0.45
2021-06-16 11:08:27,841 - INFO - joeynmt.training - EPOCH 40
2021-06-16 11:08:29,132 - INFO - joeynmt.training - Epoch  40: total training loss 0.40
2021-06-16 11:08:29,132 - INFO - joeynmt.training - EPOCH 41
2021-06-16 11:08:30,479 - INFO - joeynmt.training - Epoch  41: total training loss 0.50
2021-06-16 11:08:30,479 - INFO - joeynmt.training - EPOCH 42
2021-06-16 11:08:31,811 - INFO - joeynmt.training - Epoch  42: total training loss 0.41
2021-06-16 11:08:31,811 - INFO - joeynmt.training - EPOCH 43
2021-06-16 11:08:33,163 - INFO - joeynmt.training - Epoch  43: total training loss 0.39
2021-06-16 11:08:33,164 - INFO - joeynmt.training - EPOCH 44
2021-06-16 11:08:34,449 - INFO - joeynmt.training - Epoch  44: total training loss 0.34
2021-06-16 11:08:34,450 - INFO - joeynmt.training - EPOCH 45
2021-06-16 11:08:35,652 - INFO - joeynmt.training - Epoch  45: total training loss 0.35
2021-06-16 11:08:35,652 - INFO - joeynmt.training - EPOCH 46
2021-06-16 11:08:36,995 - INFO - joeynmt.training - Epoch  46: total training loss 0.31
2021-06-16 11:08:36,995 - INFO - joeynmt.training - EPOCH 47
2021-06-16 11:08:38,379 - INFO - joeynmt.training - Epoch  47: total training loss 0.30
2021-06-16 11:08:38,379 - INFO - joeynmt.training - EPOCH 48
2021-06-16 11:08:39,701 - INFO - joeynmt.training - Epoch  48: total training loss 0.28
2021-06-16 11:08:39,701 - INFO - joeynmt.training - EPOCH 49
2021-06-16 11:08:41,034 - INFO - joeynmt.training - Epoch  49: total training loss 0.29
2021-06-16 11:08:41,035 - INFO - joeynmt.training - EPOCH 50
2021-06-16 11:08:42,631 - INFO - joeynmt.training - Epoch  50, Step:      200, Batch Loss:     0.059335, Tokens per Sec:      152, Lr: 0.000200
2021-06-16 11:08:42,632 - INFO - joeynmt.training - Epoch  50: total training loss 0.27
2021-06-16 11:08:42,632 - INFO - joeynmt.training - EPOCH 51
2021-06-16 11:08:43,953 - INFO - joeynmt.training - Epoch  51: total training loss 0.26
2021-06-16 11:08:43,953 - INFO - joeynmt.training - EPOCH 52
2021-06-16 11:08:45,561 - INFO - joeynmt.training - Epoch  52: total training loss 0.26
2021-06-16 11:08:45,561 - INFO - joeynmt.training - EPOCH 53
2021-06-16 11:08:47,158 - INFO - joeynmt.training - Epoch  53: total training loss 0.25
2021-06-16 11:08:47,158 - INFO - joeynmt.training - EPOCH 54
2021-06-16 11:08:48,561 - INFO - joeynmt.training - Epoch  54: total training loss 0.25
2021-06-16 11:08:48,562 - INFO - joeynmt.training - EPOCH 55
2021-06-16 11:08:49,956 - INFO - joeynmt.training - Epoch  55: total training loss 0.24
2021-06-16 11:08:49,956 - INFO - joeynmt.training - EPOCH 56
2021-06-16 11:08:51,274 - INFO - joeynmt.training - Epoch  56: total training loss 0.25
2021-06-16 11:08:51,274 - INFO - joeynmt.training - EPOCH 57
2021-06-16 11:08:52,597 - INFO - joeynmt.training - Epoch  57: total training loss 0.28
2021-06-16 11:08:52,597 - INFO - joeynmt.training - EPOCH 58
2021-06-16 11:08:53,922 - INFO - joeynmt.training - Epoch  58: total training loss 0.23
2021-06-16 11:08:53,922 - INFO - joeynmt.training - EPOCH 59
2021-06-16 11:08:55,244 - INFO - joeynmt.training - Epoch  59: total training loss 0.25
2021-06-16 11:08:55,244 - INFO - joeynmt.training - EPOCH 60
2021-06-16 11:08:56,577 - INFO - joeynmt.training - Epoch  60: total training loss 0.24
2021-06-16 11:08:56,577 - INFO - joeynmt.training - EPOCH 61
2021-06-16 11:08:57,923 - INFO - joeynmt.training - Epoch  61: total training loss 0.27
2021-06-16 11:08:57,923 - INFO - joeynmt.training - EPOCH 62
2021-06-16 11:08:59,227 - INFO - joeynmt.training - Epoch  62: total training loss 0.25
2021-06-16 11:08:59,227 - INFO - joeynmt.training - EPOCH 63
2021-06-16 11:09:00,546 - INFO - joeynmt.training - Epoch  63: total training loss 0.23
2021-06-16 11:09:00,546 - INFO - joeynmt.training - EPOCH 64
2021-06-16 11:09:01,889 - INFO - joeynmt.training - Epoch  64: total training loss 0.23
2021-06-16 11:09:01,889 - INFO - joeynmt.training - EPOCH 65
2021-06-16 11:09:03,304 - INFO - joeynmt.training - Epoch  65: total training loss 0.23
2021-06-16 11:09:03,304 - INFO - joeynmt.training - EPOCH 66
2021-06-16 11:09:04,919 - INFO - joeynmt.training - Epoch  66: total training loss 0.23
2021-06-16 11:09:04,919 - INFO - joeynmt.training - EPOCH 67
2021-06-16 11:09:06,294 - INFO - joeynmt.training - Epoch  67: total training loss 0.22
2021-06-16 11:09:06,294 - INFO - joeynmt.training - EPOCH 68
2021-06-16 11:09:07,681 - INFO - joeynmt.training - Epoch  68: total training loss 0.22
2021-06-16 11:09:07,681 - INFO - joeynmt.training - EPOCH 69
2021-06-16 11:09:08,999 - INFO - joeynmt.training - Epoch  69: total training loss 0.24
2021-06-16 11:09:08,999 - INFO - joeynmt.training - EPOCH 70
2021-06-16 11:09:10,290 - INFO - joeynmt.training - Epoch  70: total training loss 0.21
2021-06-16 11:09:10,290 - INFO - joeynmt.training - EPOCH 71
2021-06-16 11:09:11,640 - INFO - joeynmt.training - Epoch  71: total training loss 0.22
2021-06-16 11:09:11,640 - INFO - joeynmt.training - EPOCH 72
2021-06-16 11:09:12,958 - INFO - joeynmt.training - Epoch  72: total training loss 0.23
2021-06-16 11:09:12,958 - INFO - joeynmt.training - EPOCH 73
2021-06-16 11:09:14,262 - INFO - joeynmt.training - Epoch  73: total training loss 0.25
2021-06-16 11:09:14,262 - INFO - joeynmt.training - EPOCH 74
2021-06-16 11:09:15,651 - INFO - joeynmt.training - Epoch  74: total training loss 0.22
2021-06-16 11:09:15,652 - INFO - joeynmt.training - EPOCH 75
2021-06-16 11:09:17,031 - INFO - joeynmt.training - Epoch  75, Step:      300, Batch Loss:     0.034577, Tokens per Sec:      176, Lr: 0.000200
2021-06-16 11:09:17,032 - INFO - joeynmt.training - Epoch  75: total training loss 0.20
2021-06-16 11:09:17,032 - INFO - joeynmt.training - EPOCH 76
2021-06-16 11:09:18,342 - INFO - joeynmt.training - Epoch  76: total training loss 0.20
2021-06-16 11:09:18,343 - INFO - joeynmt.training - EPOCH 77
2021-06-16 11:09:19,581 - INFO - joeynmt.training - Epoch  77: total training loss 0.19
2021-06-16 11:09:19,581 - INFO - joeynmt.training - EPOCH 78
2021-06-16 11:09:20,814 - INFO - joeynmt.training - Epoch  78: total training loss 0.19
2021-06-16 11:09:20,814 - INFO - joeynmt.training - EPOCH 79
2021-06-16 11:09:22,125 - INFO - joeynmt.training - Epoch  79: total training loss 0.17
2021-06-16 11:09:22,127 - INFO - joeynmt.training - EPOCH 80
2021-06-16 11:09:23,563 - INFO - joeynmt.training - Epoch  80: total training loss 0.17
2021-06-16 11:09:23,564 - INFO - joeynmt.training - EPOCH 81
2021-06-16 11:09:25,434 - INFO - joeynmt.training - Epoch  81: total training loss 0.17
2021-06-16 11:09:25,434 - INFO - joeynmt.training - EPOCH 82
2021-06-16 11:09:26,807 - INFO - joeynmt.training - Epoch  82: total training loss 0.17
2021-06-16 11:09:26,807 - INFO - joeynmt.training - EPOCH 83
2021-06-16 11:09:28,112 - INFO - joeynmt.training - Epoch  83: total training loss 0.17
2021-06-16 11:09:28,113 - INFO - joeynmt.training - EPOCH 84
2021-06-16 11:09:29,550 - INFO - joeynmt.training - Epoch  84: total training loss 0.17
2021-06-16 11:09:29,550 - INFO - joeynmt.training - EPOCH 85
2021-06-16 11:09:30,852 - INFO - joeynmt.training - Epoch  85: total training loss 0.17
2021-06-16 11:09:30,852 - INFO - joeynmt.training - EPOCH 86
2021-06-16 11:09:32,182 - INFO - joeynmt.training - Epoch  86: total training loss 0.16
2021-06-16 11:09:32,182 - INFO - joeynmt.training - EPOCH 87
2021-06-16 11:09:33,521 - INFO - joeynmt.training - Epoch  87: total training loss 0.16
2021-06-16 11:09:33,521 - INFO - joeynmt.training - EPOCH 88
2021-06-16 11:09:34,788 - INFO - joeynmt.training - Epoch  88: total training loss 0.15
2021-06-16 11:09:34,788 - INFO - joeynmt.training - EPOCH 89
2021-06-16 11:09:36,439 - INFO - joeynmt.training - Epoch  89: total training loss 0.16
2021-06-16 11:09:36,440 - INFO - joeynmt.training - EPOCH 90
2021-06-16 11:09:38,161 - INFO - joeynmt.training - Epoch  90: total training loss 0.16
2021-06-16 11:09:38,161 - INFO - joeynmt.training - EPOCH 91
2021-06-16 11:09:39,623 - INFO - joeynmt.training - Epoch  91: total training loss 0.16
2021-06-16 11:09:39,623 - INFO - joeynmt.training - EPOCH 92
2021-06-16 11:09:41,028 - INFO - joeynmt.training - Epoch  92: total training loss 0.15
2021-06-16 11:09:41,028 - INFO - joeynmt.training - EPOCH 93
2021-06-16 11:09:42,377 - INFO - joeynmt.training - Epoch  93: total training loss 0.15
2021-06-16 11:09:42,377 - INFO - joeynmt.training - EPOCH 94
2021-06-16 11:09:43,699 - INFO - joeynmt.training - Epoch  94: total training loss 0.16
2021-06-16 11:09:43,699 - INFO - joeynmt.training - EPOCH 95
2021-06-16 11:09:44,962 - INFO - joeynmt.training - Epoch  95: total training loss 0.15
2021-06-16 11:09:44,962 - INFO - joeynmt.training - EPOCH 96
2021-06-16 11:09:46,228 - INFO - joeynmt.training - Epoch  96: total training loss 0.16
2021-06-16 11:09:46,229 - INFO - joeynmt.training - EPOCH 97
2021-06-16 11:09:47,605 - INFO - joeynmt.training - Epoch  97: total training loss 0.14
2021-06-16 11:09:47,605 - INFO - joeynmt.training - EPOCH 98
2021-06-16 11:09:48,865 - INFO - joeynmt.training - Epoch  98: total training loss 0.15
2021-06-16 11:09:48,865 - INFO - joeynmt.training - EPOCH 99
2021-06-16 11:09:50,324 - INFO - joeynmt.training - Epoch  99: total training loss 0.14
2021-06-16 11:09:50,325 - INFO - joeynmt.training - EPOCH 100
2021-06-16 11:09:51,754 - INFO - joeynmt.training - Epoch 100, Step:      400, Batch Loss:     0.040529, Tokens per Sec:      170, Lr: 0.000200
2021-06-16 11:09:51,754 - INFO - joeynmt.training - Epoch 100: total training loss 0.15
2021-06-16 11:09:51,755 - INFO - joeynmt.training - Training ended after 100 epochs.
2021-06-16 11:09:51,755 - INFO - joeynmt.training - Best validation result (greedy) at step        0:    inf ppl.
2021-06-16 11:09:51,771 - INFO - joeynmt.prediction - Process device: cpu, n_gpu: 0, batch_size per device: 500 (with beam_size)
